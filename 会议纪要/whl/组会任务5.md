- 小组准备
  1. 建立Github小组公共项目库
     - 建立论文及会议报告文件夹
     - 收集目标检测数据集
     - 收集实用链接及项目库
  2. GPU到手
     - 下载xShell，xFip等软件
     - 了解Slurm中的sbatch, salloc, srun等语言
       - sbatch 用于提交批处理作业，不可交互，用于长作业内容
       - salloc 用于申请节点，申请到之后用cmd交互
       - srun 用于直接运行实时性程序，命令无记忆
     - 已能够成功链接到集群，可以实现传输文件，编写运行脚本等



- 学术方面

  1. essay
     - end-to-end detection with transformers
       1. 性能影响因素（包括训练和推理的性能）
          - postprocessing steps to colalpse near-duplicate predictions（后处理步骤以合并近似重复的预测）
          - design of the anchor set（锚点集的设计）
          - heuristics that assign target boxes to anchors（将目标框分配给锚点的启发式方法）
       2. Transformer优势
          - architecture for sequence prediction（用于序列预测的架构）
          - explicitly model all pairwise interactions between elements in a sequence, fit for specific constraints of set prediction such as removing duplicate predictions（明确地对序列中元素之间的所有成对交互进行建模，适用于集合预测的特定约束，例如去除重复预测）
          - predict all objects at once（一次性预测所有对象）
          - simplify the detection pipeline by dropping multiple hand-designed components（通过删除多个手动设计的组件来简化检测流程）
          - don't require any customized layers, can be reproduced easily in any framework that contains standard CNN and transformer classes（不需要任何定制层，可以轻松在任何包含标准CNN和Transformer类的框架中复现）
       3. realize
          1. 特征处理，使用cnn神经网络将图像提取成向量特征喂给transformer
          2. encoder，通过多注意力头逐层将从cnn提取到的特征向量编码，实现重点关注
          3. decoder，每个解码器具有两个输入，分别是encoder的输出和上一个层解码器的输出，从而使得每次输出与原图像相关，增加上下文理解。
          4. 根据解码向量输出结果

     - practice

       1. **pretrain detr model (ResNet-50)**

          ```python
          import time
          from transformers import DetrImageProcessor, DetrForObjectDetection
          import torch
          from PIL import Image
          import matplotlib.pyplot as plt
          import matplotlib.patches as patches
          
          # 加载一张图像，并将其转换为RGB格式
          image = Image.open("屏幕截图 2023-10-08 224329.png").convert("RGB")
          
          # 使用预训练的DETR模型和处理器
          # 图像处理
          processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
          # 对象检测
          model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
          
          # 使用处理器来准备图像输入。它将图像转换为模型可以理解的格式（例如，它可能对图像进行归一化、缩放等）。
          # 'return_tensors="pt"' 指示我们希望返回的张量类型为PyTorch张量。
          '''
          张量是一个多维数组，它是标量、向量和矩阵的泛化。在深度学习中，张量通常用于存储和处理数据。例如，在图像处理中，一张彩色图像通常表示为一个形状为[H,W,C] 的三维张量，其中 H 是图像的高度，W 是图像的宽度，C 是通道数（通常为3，代表RGB通道）。
          '''
          inputs = processor(images=image, return_tensors="pt")
          # 输出模型结果
          outputs = model(**inputs)
          
          # size[::-1]是用来反转图像大小的元组的（宽度, 高度）->（高度, 宽度）。
          target_sizes = torch.tensor([image.size[::-1]])
          
          # 将模型的原始输出转换为实际的边界框坐标、筛选出得分较高的预测等。
          # 阈值（threshold）用于筛选出得分超过该阈值的预测。
          results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.2)[0]
          
          for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
              box = [round(i, 2) for i in box.tolist()]
          
          plt.imshow(image)
          
          # 将每个检测的边界框画在图片上
          for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
              box = [round(i, 2) for i in box.tolist()]
              rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='g',
                                       facecolor='none')
              plt.gca().add_patch(rect)
              plt.text(box[0], box[1] - 2, f"{model.config.id2label[label.item()]}: {round(score.item(), 3)}", color='red')
          
          plt.show()
          ```

          ![7b45a5a36a6c26fa1b050e073bca379](C:\Users\14815\Documents\WeChat Files\wxid_hs7bm9yapf5x12\FileStorage\Temp\7b45a5a36a6c26fa1b050e073bca379.png)

          ```py
          threshold = 0.2
          Import Time: 3.3862903118133545
          Process Time: 0.9557785987854004
          
          Detected person with confidence 0.506 at location [1021.82, 613.08, 1497.37, 1271.34]
          Detected backpack with confidence 0.206 at location [1613.8, 178.47, 1956.74, 517.48]
          Detected person with confidence 0.732 at location [1560.58, 501.23, 2005.43, 1165.51]
          Detected person with confidence 0.248 at location [881.93, 617.12, 1510.77, 1307.94]
          Detected person with confidence 0.289 at location [1010.71, 43.51, 2020.78, 1185.61]
          Detected person with confidence 0.264 at location [1005.42, 31.49, 1845.59, 1193.18]
          Detected person with confidence 0.364 at location [1041.43, 509.34, 1992.61, 1196.92]
          ```

          ![image-20231012135519915](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012135519915.png)

          ```py
          threshold = 0.5
          Process Time: 0.9441683292388916
          ```

          ![image-20231012140620713](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012140620713.png)

          ```py
          threshold = 0.8
          Time: 1.0510485172271729
          ```

          

       2. **Faster R-CNN**

          ```py
          import torch
          from torchvision.models.detection import fasterrcnn_resnet50_fpn
          from torchvision.transforms import functional as F
          from PIL import Image
          import matplotlib.pyplot as plt
          import matplotlib.patches as patches
          import time
          
          # 加载模型
          model = fasterrcnn_resnet50_fpn(pretrained=True)
          model.eval()
          
          # 加载图像
          image_path = 'materials/images/屏幕截图 2023-10-08 224329.png'  # 替换为你的图像路径
          image = Image.open(image_path).convert("RGB")
          
          # 图像转换为张量
          image_tensor = F.to_tensor(image).unsqueeze(0)
          
          # 使用模型进行预测
          start_time = time.time()
          with torch.no_grad():
              prediction = model(image_tensor)
          print(f"Time: {time.time() - start_time}")
          
          # 可视化结果
          fig, ax = plt.subplots(1, figsize=(12, 9))
          ax.imshow(image)
          
          # 显示检测框和标签
          boxes = prediction[0]['boxes'].numpy()
          scores = prediction[0]['scores'].numpy()
          labels = prediction[0]['labels'].numpy()
          
          # 设置一个阈值，只显示置信度高于该值的检测框
          score_threshold = 0.4
          COCO_INSTANCE_CATEGORY_NAMES = [
              '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
              'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
              'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
              'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
              'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
              'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
              'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
              'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
              'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
              'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
              'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
              'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
          ]
          
          for box, score, label in zip(boxes, scores, labels):
              if score >= score_threshold:
                  category_name = COCO_INSTANCE_CATEGORY_NAMES[label]
                  display_text = f"{category_name}: {score:.2f}"
                  rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='r',
                                           facecolor='none')
                  ax.add_patch(rect)
                  ax.text(box[0], box[1], display_text, fontsize=10,
                          bbox=dict(facecolor='yellow', alpha=0.5, edgecolor='none', boxstyle='round,pad=0.1'))
          
          plt.axis('off')
          plt.show()
          ```

          ![image-20231012135733409](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012135733409.png![image-20231012140012978](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012140012978.png)

          ```py
          threshold = 0.5
          process time: 1.2525696754455566
          ```

          ![image-20231012140724483](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012140724483.png)

          ```py
          threshold = 0.8
          Time: 1.3565123081207275
          ```

       3. YOLO-tiny

          ```python
          from transformers import YolosImageProcessor, YolosForObjectDetection
          from PIL import Image
          import torch
          import time
          import matplotlib.pyplot as plt
          import matplotlib.patches as patches
          
          # url = "http://images.cocodataset.org/val2017/000000039769.jpg"
          # image = Image.open(requests.get(url, stream=True).raw)
          image_path = 'materials/images/屏幕截图 2023-10-12 140251.png'  # 替换为你的图像路径
          image = Image.open(image_path).convert("RGB")
          
          model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')
          image_processor = YolosImageProcessor.from_pretrained("hustvl/yolos-tiny")
          
          start_time = time.time()
          inputs = image_processor(images=image, return_tensors="pt")
          outputs = model(**inputs)
          print(f"Time: {time.time() - start_time}")
          
          # model predicts bounding boxes and corresponding COCO classes
          logits = outputs.logits
          bboxes = outputs.pred_boxes
          
          
          # print results
          target_sizes = torch.tensor([image.size[::-1]])
          results = image_processor.post_process_object_detection(outputs, threshold=0.8, target_sizes=target_sizes)[0]
          plt.imshow(image)
          
          for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
              box = [round(i, 2) for i in box.tolist()]
              rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='blue',
                                       facecolor='none')
              plt.gca().add_patch(rect)
              plt.text(box[0], box[1] - 2, f"{model.config.id2label[label.item()]}: {round(score.item(), 3)}", color='red')
          
          plt.show()
          ```

          ![image-20231012143502200](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231012143502200.png)

          ```py
          threshold = 0.8
          Time: 0.5235462188720703
          ```

          

          

- 扩展

  1. ## Q: 可以在训练阶段分别学习，然后整合成一个整体模型吗？

     A: 在训练阶段分别进行学习，然后将两个或多个模型整合成一个整体，是一种常见的策略。这种方法通常称为模型集成。通过集成，可以将从各个模型中得到的知识结合起来，可能会获得更好的性能和泛化能力。以下是几种常见的集成和整合策略：

     1. **模型级别的集成**:

        - **投票集成**：各个模型独立地对输入数据做出预测，然后通过投票机制确定最终的预测。

        - **平均集成**：计算各个模型的预测的平均值作为最终的预测。

        - **权重平均**：为每个模型的预测分配权重，然后计算加权平均值作为最终的预测。

     2. **特征级别的整合**:

        - 将来自不同模型的特征或预测整合在一起，然后送入一个单独的模型进行最终的预测。这种方法可以帮助合并不同模型学到的表示。

     3. **训练一个新的模型**:

        - 用前面训练好的模型的输出作为新模型的输入，或者将前面训练好的模型的权重作为新模型的初始权重，然后继续训练新模型以优化目标任务。

     4. **多任务学习**:
        - 设计一个多任务模型，该模型能同时处理多个相关任务。虽然这不是从分开的模型整合，但是它可以同时学习定位和分类等任务。

     在你的情况中，如果你首先独立地训练了定位模型和分类模型，你可能想要考虑将这两个模型的输出或特征整合在一起，并可能训练一个新的模型来同时优化定位和分类。例如，你可以使用一个简单的逻辑回归或者全连接层来整合两个模型的输出，并进行最终的预测。

  2. 在理想情况下，对于确定性的模型，如果输入数据和模型参数保持不变，那么每次运行模型对同一个图像的检测结果应该是一样的。这是因为神经网络中的所有计算（如卷积、池化和激活函数）都是确定性的，不涉及任何随机过程。

     然而，实际情况可能会稍有不同，可能会出现一些影响结果一致性的因素：

     1. **随机性**:
        - 在训练和推理过程中，有时会引入一些随机性，如 dropout 层或随机数据增强。但通常，在推理阶段，dropout 层会被禁用，以确保结果的一致性。
     2. **数值精度和舍入错误**:
        - 由于浮点数的精度限制和计算过程中的舍入错误，可能会出现微小的不一致。这种不一致通常非常小，不太可能影响检测结果。
     3. **硬件和软件差异**:
        - 不同的硬件平台和软件实现可能会导致微小的数值差异。例如，CPU和GPU可能会有不同的数值精度，不同的深度学习框架也可能有微小的实现差异。
     4. **非确定性运算**:
        - 一些特定的运算或库可能具有内在的非确定性，例如一些高度优化的并行运算。
     5. **多线程和并行计算**:
        - 在多线程或并行计算环境中，运算的顺序可能会影响结果。这可能会导致每次运行时结果的微小变化。

     在大多数情况下，这些因素的影响都是微小的，不太可能影响模型的检测结果。如果需要确保完全的一致性，可以考虑在推理时禁用所有随机操作，并确保在一个固定的硬件和软件环境中运行模型。

  3. ## Q: 我不理解作为一个架构的概念，比如cnn，detr等，他们规定了哪些，还有哪些需要自己修改

     A: 一个模型架构（例如 CNN、DETR）基本上是一个蓝图或模板，它定义了模型的基本结构和计算流程。以下是模型架构通常规定和不规定的内容：

     ### **通常由架构规定的内容**:

     1. **层的类型和顺序**:
        - 例如，CNN（卷积神经网络）通常包含卷积层、池化层和全连接层。
        - DETR（基于 Transformer 的端到端对象检测器）包含 Transformer 和一个特定的输出头用于对象检测。

     2. **连接方式**:
        - 例如，层之间是顺序连接，还是有跳跃连接（例如在残差网络中）。

     3. **前向传播的计算流程**:
        - 如何从输入数据计算输出。

     4. **损失函数的形式（有时）**:
        - 例如，DETR 提出了一个特定的 bipartite matching loss。

     ### **通常需要自己确定或修改的内容**:

     1. **超参数**:
        - 例如，层数、每层的节点数、学习率、批处理大小等。

     2. **初始化方法**:
        - 如何初始化网络的权重和偏置。

     3. **优化器选择**:
        - 如 SGD、Adam 等。

     4. **正则化技术**:
        - 如 dropout、L2 正则化等。

     5. **激活函数**:
        - 例如 ReLU、sigmoid、tanh 等。

     6. **特定层的配置**:
        - 例如，在 CNN 中卷积层的滤波器大小、步长和填充。

     7. **损失函数的选择（如果架构没有指定）**:
        - 例如，交叉熵损失、均方误差损失等。

     8. **数据预处理和数据增强策略**.

     9. **可能的架构修改和扩展**:
        - 例如，添加额外的层、修改连接方式等。

     这些自定义选项和扩展允许研究人员和开发人员根据具体的任务和数据来调整和优化模型架构。通过合适的选择和调整，可以显著影响模型的性能和泛化能力。

  4. ## Q: 具体讲讲元学习是怎么实现的，是什么

     A: 元学习（Meta-Learning）是一种“学习如何学习”的方法，它的目标是开发能够快速适应新任务的算法，尤其是在只有少量训练数据可用的情况下。元学习算法通常在多个任务上进行训练，以学习一个良好的初始化或一个能够生成任务特定学习算法的模型。以下是一些常见的元学习方法和它们的基本思想：

     ### 1. **模型无关元学习（Model-Agnostic Meta-Learning, MAML)**:
     MAML 试图找到一个好的参数初始化，使得从这个初始化开始，模型可以通过少量的梯度更新快速适应新任务。具体来说，MAML 的训练过程包括两个嵌套的循环：
     - **外循环**：在外循环中，模型在所有任务上进行训练，目标是最小化所有任务上经过少量梯度更新后的损失之和。
     - **内循环**：在内循环中，模型对每个任务进行少量的梯度更新，通常只有一步或几步。

     ### 2. **元网络（Meta-Networks)**:
     元网络试图学习一个网络，它可以为每个任务生成任务特定的权重。这通常是通过训练一个额外的网络（元网络）来实现的，它可以根据任务的描述或示例生成主网络的权重。

     ### 3. **学习优化器（Learning to Optimize)**:
     在这种方法中，元学习算法试图学习一个优化器，它可以为每个任务生成一个优化算法。这是通过训练一个网络来实现的，该网络可以产生一个优化算法，该算法在多个任务上表现良好。

     ### 4. **条件神经过程（Conditional Neural Processes, CNPs)**:
     CNPs 是一种尝试学习任务间潜在关系的方法，它们通过条件编码器来学习从任务描述或示例到潜在空间的映射，并通过解码器来生成任务特定的预测。

     ### 5. **重新使用先验知识（Reusing Prior Knowledge）**:
     通过在相关任务上预训练模型，然后将预训练的模型作为新任务的初始化或作为特征提取器来重用先验知识。

     ### 6. **黑箱自适应系统（Black-box Adaptation Systems）**:
     黑箱自适应系统通过直接学习从任务描述或示例到模型参数的映射来实现快速适应。

     元学习是一个非常活跃且多样化的研究领域，其核心是使机器学习模型能够更有效地适应新的、未见过的任务，尤其是在只有少量标签数据的情况下。

  5. ## Q: 对于复杂场景的小目标检测有哪些可能的创新点

     A:复杂场景下的小目标检测是计算机视觉领域的一个挑战性问题，因为小目标在图像中占据的像素较少，很容易受到遮挡和噪声的影响。以下是一些可能的研究方向和创新点：

     1. **特征增强和融合**：
        - 研究如何提取和增强小目标的特征表示，例如通过多尺度特征融合、上采样等方法。
        - 探索不同层次和类型的特征融合方法，以充分利用模型中的多层特征信息。

     2. **注意力机制**：
        - 集成注意力机制，使模型能够更加关注小目标区域，减小背景干扰和遮挡的影响。
        - 研究动态和自适应的注意力分配策略，以适应不同场景和目标的变化。

     3. **数据增强和合成**：
        - 开发针对小目标检测的数据增强方法，例如随机缩放、裁剪、旋转等，以增加模型的鲁棒性。
        - 利用生成模型，如GANs，合成具有小目标的训练样本，以扩展数据集并改善模型性能。

     4. **上下文和关系建模**：
        - 利用小目标所在场景的上下文信息，例如背景、邻近物体等，来辅助小目标的检测。
        - 建模目标之间的关系和交互，以利用群体信息和结构信息提高检测性能。

     5. **多模态和多源信息融合**：
        - 探索如何融合来自不同传感器和模态的信息，例如RGB图像、深度图、热成像等，以提高小目标检测的准确性和鲁棒性。
        - 研究多模态信息的对齐、匹配和融合方法。

     6. **在线学习和模型更新**：
        - 开发在线学习算法，使模型能够实时适应场景变化和新出现的小目标。
        - 研究模型增量更新、迁移学习等方法，以提高模型的适应性和长期性能。

     7. **硬件和计算效率优化**：
        - 研究轻量化网络结构和模型压缩方法，以在资源有限的环境中实现小目标检测。
        - 探索模型在特定硬件平台上的优化和加速方法。

     8. **鲁棒性和对抗性研究**：
        - 研究如何提高模型对不利条件（例如光照变化、遮挡、模糊等）的鲁棒性。
        - 探索模型对对抗攻击的防御方法和策略。

     9. **评估标准和基准测试**：
        - 提出针对小目标检测的新的评估标准和性能指标。
        - 构建小目标检测的基准数据集和测试平台，以推动研究和技术的发展。

     在这些方向中进行创新和深入研究，有可能提出新的方法和技术，解决复杂场景下小目标检测的挑战，并有机会发表高质量的论文。
     
  6. ### Q: 有哪些目标检测方法，并打分
  
     1. **基于Anchor的方法**:
        - **Faster R-CNN**: 这是一种两阶段检测器，首先生成一系列的提议框，然后对其进行分类和回归。
        - **YOLO (You Only Look Once)**: YOLO系列是单阶段检测器，它直接预测边界框和类别。
        - **SSD (Single Shot MultiBox Detector)**: 类似于YOLO，但使用多尺度特征图来检测不同大小的物体。
        - **EfficientDet**: 结合了EfficientNet的骨干网络和一系列的优化技巧，旨在实现高效而强大的目标检测。
     2. **基于Transformer的方法**:
        - **DETR (DEtection TRansformer)**: 如前所述，DETR使用Transformer结构进行端到端的目标检测。
        - **BEiT (BERT-IN-THE-WILD)**: 结合了ViT和BERT的思路，利用大量的未标注数据进行预训练，再在目标检测任务上进行微调。
     3. **其他方法**:
        - **CenterNet**: 使用关键点检测的方法来检测物体的中心，并从中心推断出边界框。
        - **FCOS (Fully Convolutional One-Stage Object Detection)**: 是一个不使用Anchor的检测器，直接从特征图预测边界框和类别。
     4. **神经网络架构搜索 (NAS)**:
        - 一些方法使用NAS来自动搜索最佳的目标检测架构，例如NAS-FPN和EfficientDet
  
     | 模型         | 速度 | 大目标准确率 | 小目标准确率 |
     | ------------ | ---- | ------------ | ------------ |
     | Faster R-CNN | 2    | 5            | 4            |
     | YOLO         | 5    | 4            | 3            |
     | SSD          | 4    | 4            | 3            |
     | EfficientDet | 4    | 5            | 5            |
     | DETR         | 3    | 4            | 4            |
     | BEiT         | 3    | 4            | 4            |
     | CenterNet    | 4    | 4            | 4            |
     | FCOS         | 4    | 4            | 4            |



- 问题
  - 关于实用场景，攀登计划
  - 


















- 思考
  1. 模拟人类通过景深信息学习
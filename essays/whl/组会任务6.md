### DINO的基本特性

1. **视觉表示学习**：DINO的目标是学习视觉表示，这些表示可以捕获图像中的重要信息，而无需依赖手动标记的训练数据。这通过训练模型来识别不同视图之间的相似性和差异性来实现，这些视图是从同一图像（使用数据增强技术）或不同图像中获得的。

2. **对比学习**：许多自监督方法依赖于对比学习，其中模型需要区分不同图像的表示。然而，DINO不直接比较样本之间的相似性。相反，它使用了一个教师模型和一个学生模型，学生模型学习通过与教师模型的输出对齐其输出来提取特征。

3. **教师-学生框架**：在DINO中，学生网络试图使它的视觉表示靠近教师网络的视觉表示。关键是，教师网络不是固定的；它是一个动量编码器，这意味着它的权重是其自身历史权重和学生网络当前权重的组合。这有助于稳定训练过程并提高学生网络学习有用表示的能力。

4. **自注意力机制**：DINO还利用了自注意力机制来增强学生模型的视觉表示能力。通过自注意力，模型可以更有效地关注图像中的重要区域，并从整个图像上下文中学习。

5. **数据增强和多样性**：通过对输入图像进行各种数据增强（例如，裁剪、颜色扭曲等），DINO能够从多样性视图中学习，这对于学习强大的、泛化的视觉表示至关重要。

   

### 与detr区别

DINO（Dense Image Navigation with Self-supervision）和DETR（Detection Transformer）都利用了Transformer结构，但它们用于不同的目的和不同的任务类型。让我们分别看看这两者的主要差异：

1. **目标和应用**:
   - **DINO** 是一个自监督学习方法，旨在从未标记的数据中学习强大的视觉表示。它不直接进行目标检测或图像分类；相反，它学习的特征表示可以被用作后续任务（如分类、检测、分割等）的预训练模型。
   - **DETR** 是一个用于目标检测的端到端框架，它使用Transformer来直接从图像中检测和识别对象。不同于传统的目标检测方法（如Faster R-CNN），DETR避免了使用手工定义的部件（例如锚点和区域提议网络），而是用一个集成的Transformer模型来处理整个任务。
2. **模型架构和方法**:
   - **DINO** 主要利用一个教师-学生架构，其中两个Transformer网络（学生和教师）通过对未标记数据的共同处理来学习视觉表示。DINO专注于使学生模型的输出与教师模型的输出对齐，而不涉及具体的目标检测任务。
   - **DETR** 使用一个标准的Transformer结构，并将其与CNN（通常是ResNet）相结合作为一个后端来提取图像特征。然后，它使用Transformer的编码器-解码器架构来解析这些特征并直接输出一组预测的边界框和类别。
3. **训练数据和监督**:
   - **DINO** 不依赖于带标签的数据进行训练。它通过比较来自同一图像的不同视图来实现自监督学习，学习它们之间的一致性。
   - **DETR** 则需要标记的训练数据，包括对象的边界框和类别标签，因为它是一个监督学习任务，旨在直接预测这些信息。
4. **输出**:
   - **DINO** 的输出是图像的特征表示，这些表示可以被进一步用于各种视觉任务。
   - **DETR** 的输出是一组对象的边界框、类别和分数，直接从单个网络推断得出，不需要传统的目标检测流水线的组成部分。



### 学习阶段：

1. **数据准备**:

   - **输入**：DINO接受无标签的图像作为输入。
   - **数据处理**：
   - ![image-20231026145535382](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231026145535382.png)
     1. 分为筛选（质量高）和未筛选数据集（爬虫等）
     2. 将图片映射到向量空间
     3. 去除未筛选数据集中相似的图像
     4. 选取未筛选数据集中与筛选数据集中相似的图像
     5. 得到增广数据集，扩大数据量和质量

2. **数据增强**：为了训练模型，从每个图像生成两个独立的视图（即两个经过变换的版本），通常通过随机裁剪、颜色扰动、模糊等技术实现。

   - DINO 中最核心的数据采样策略便是图像裁剪，这也是自监督学习领域应用非常广泛的主策略之一。一般来说，我们可以将裁剪后的图像分为两种：

     - `Local views`: 即**局部视角**，也称为 small crops，指的是抠图面积小于原始图像的 50%；
     - `Global views`: 即**全局视角**，也称为 large crops，指的是抠图面积大于原始图像的 50%；

     在 DINO 中，学生模型接收所有预处理过的 crops 图，而教师模型仅接收来自 global views 的裁剪图。据作者称，这是为了鼓励从局部到全局的响应，从而训练学生模型从一个小的裁剪画面中推断出更广泛的上下文信息。

3. **教师-学生架构**:
   - DINO采用了一个特别的教师-学生架构。这里，“教师”和“学生”都是神经网络，但具有不同的角色。
   - **学生模型**：此模型进行权重更新和学习。它试图学习一个可以提取图像视图的表示的功能，这些表示捕捉了视觉数据的基本结构和模式。
   - **教师模型**：教师模型是通过对学生模型权重的指数移动平均（EMA）来维护的，这有助于平滑学生学习过程中的噪声并提供更稳定的目标。重要的是，教师模型不是直接学习的，而是随着学生模型的学习而间接地进行更新。

   1. **初始化**：教师模型和学生模型开始时具有相同的网络架构和权重。然而，它们的目的和功能随着时间的推移而变化。

   2. **数据处理和增强**：对于每个输入图像，都会创建两个不同的视图（通过裁剪、调整颜色/对比度等）。这些视图被同时提供给教师和学生模型。

   3. **前向传播**：学生和教师模型都通过其网络处理这些视图，产生高维特征表示。

   4. **自监督学习目标**：学生模型被训练以最小化其输出特征表示与教师模型的对应表示之间的某种距离。这促使学生模型学习产生与教师类似的表示。

   5. **权重更新**：

      - **学生模型**：通过标准的梯度下降方法，学生模型的权重被直接更新以减少其输出与教师的差异。
      - **教师模型**：不是通过反向传播进行权重更新的，而是通过对学生模型权重的移动平均来更新的。这种方法确保了教师模型的稳定性和平滑性，这对于防止学生模型过度拟合到特定的数据视图是重要的。

   6. **教师和学生网络是怎么互相影响的，不能只训练教师成了后再训练学生吗**
      在DINO中，教师网络和学生网络之间存在一种互相影响的关系，它们之间的交互是这个方法的关键部分。以下是它们是如何互相影响的：

      1. **教师网络的影响**：教师网络为学生网络提供一个“目标”。通过对输入数据的前向传播，教师网络生成一个稳定的输出，学生网络的任务是生成一个接近这个输出的表示。这确保了学生网络在训练过程中有一个清晰、稳定的目标，使得其学习过程更加稳定。
      2. **学生网络的影响**：学生网络的权重在每一次迭代中都会更新，它的输出会与教师网络的输出进行对比以计算损失。随后，根据这个损失来更新学生网络的权重。然后，教师网络的权重也会被更新，但是这种更新是通过对学生网络的权重进行动量平均来实现的，而不是直接基于损失。

      如果我们只先训练教师网络，然后再训练学生网络，这将违反这种自我监督学习的精神。教师网络和学生网络的持续交互提供了一种机制，使学生网络可以不断地根据教师网络的输出进行调整，并逐渐学习到有意义的表示。如果我们分开训练它们，那么学生网络就失去了一个稳定的、动态适应的参考，这会使得其学习过程变得不稳定和效果较差。

4. **学习过程**:
   - 学生模型处理两个增强的图像视图，并试图调整其参数，使得这两个视图在特征空间中彼此接近（即具有相似的内部表示）。
   - 同时，学生模型的输出还尝试接近教师模型的输出。但不同于传统的对比学习方法，DINO不要求学生对其他图像样本或负样本进行区分。
   - 教师模型的权重通过学生模型的权重的EMA慢慢更新，但它不直接参与反向传播或权重优化。

5. **自我注意和正则化**:
   - DINO还利用了自注意力机制，这意味着网络可以“关注”输入视图中的不同区域，并赋予这些区域不同的重要性。
   - 为了防止学生模型简单地复制教师的输出（也就是说，防止输出变得太相似），DINO引入了正则化策略，如中心化学生的预测和/或使用温度参数来调整学生和教师输出之间的软max相似度。

6. **防止模式崩塌**

   - 当网络学习到一组特征表示时，往往会出现多个输入数据映射到相同的特征表示的情况，这就是所谓的模式崩塌。这种现象通常是由于网络在优化过程中陷入了局部最优解，只能考虑到一部分数据的特征表示，而忽略了其它数据样本的模式和特征，从而导致了多样性缺失的现象，因此会对模型的鲁棒性产生很大的负面影响。

   - centering: 通过在每次的更新值时候把激活值中心化，使得每次的处于正负空间中的激活值的特征不同。而在使用softmax（将一个非标准化的输出向量转化为概率分布）处理负值的时候通常会给较小的概率，而在处理正数时会给出较大的概率值，从而防止任何一个特征占据统治地位。
   - Sharpening: 通过在 softmax 函数中加入一个 temperature 参数，来强制让模型将概率分布更加尖锐化。由于小差异会被夸大，这会防止所有激活值都是相同的



#### 教师-学生算法：

1. **初始化**：教师模型和学生模型开始时具有相同的网络架构和权重。然而，它们的目的和功能随着时间的推移而变化。

2. **数据处理和增强**：对于每个输入图像，都会创建两个不同的视图（通过裁剪、调整颜色/对比度等）。这些视图被同时提供给教师和学生模型。

3. **前向传播**：学生和教师模型都通过其网络处理这些视图，产生高维特征表示。

4. **自监督学习目标**：学生模型被训练以最小化其输出特征表示与教师模型的对应表示之间的某种距离（例如，余弦相似度）。这促使学生模型学习产生与教师类似的表示。

5. **权重更新**：
   - **学生模型**：通过标准的梯度下降方法，学生模型的权重被直接更新以减少其输出与教师的差异。
   - **教师模型**：不是通过反向传播进行权重更新的，而是通过对学生模型权重的移动平均来更新的。这种方法确保了教师模型的稳定性和平滑性，这对于防止学生模型过度拟合到特定的数据视图是重要的。
   
6. **教师和学生网络是怎么互相影响的，不能只训练教师成了后再训练学生吗**
   在DINO中，教师网络和学生网络之间存在一种互相影响的关系，它们之间的交互是这个方法的关键部分。以下是它们是如何互相影响的：

   1. **教师网络的影响**：教师网络为学生网络提供一个“目标”。通过对输入数据的前向传播，教师网络生成一个稳定的输出，学生网络的任务是生成一个接近这个输出的表示。这确保了学生网络在训练过程中有一个清晰、稳定的目标，使得其学习过程更加稳定。
   2. **学生网络的影响**：学生网络的权重在每一次迭代中都会更新，它的输出会与教师网络的输出进行对比以计算损失。随后，根据这个损失来更新学生网络的权重。然后，教师网络的权重也会被更新，但是这种更新是通过对学生网络的权重进行动量平均来实现的，而不是直接基于损失。

   如果我们只先训练教师网络，然后再训练学生网络，这将违反这种自我监督学习的精神。教师网络和学生网络的持续交互提供了一种机制，使学生网络可以不断地根据教师网络的输出进行调整，并逐渐学习到有意义的表示。如果我们分开训练它们，那么学生网络就失去了一个稳定的、动态适应的参考，这会使得其学习过程变得不稳定和效果较差。



### 预测阶段：

- **目标检测**：要使用DINO进行目标检测，你通常需要在DINO学到的特征之上添加一个目标检测头（比如，用于预测边界框和类别的额外网络层）。这可能涉及利用DINO特征提取器的输出作为输入，然后训练一个能够根据这些特征识别和定位图像中物体的检测模型。
- **语义分割**：同样，对于语义分割，你可能需要添加一个分割头（例如，一个全卷积网络），它将DINO的输出作为输入，并产生一个像素级的分类地图，表示图像中每个像素属于哪个类别。



# 代码解读

- 整体伪代码

  ![image-20231026144514070](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231026144514070.png)

1. **变量定义**:

   - `gs, gt`: 学生和教师网络。
   - `C`: 中心（可能是特征向量的均值）。
   - `tps, tpt`: 学生和教师的温度参数。在 softmax 函数中使用，以调整概率分布的锐利度。
   - `l, m`: 网络和中心的动量率。用于更新参数。

2. **初始化**:

   - `gt.params = gs.params`: 教师网络的参数被初始化为学生网络的参数。

3. **数据加载与增强**:

   - 对于每个 mini-batch `x`:
     - `x1, x2 = augment(x), augment(x)`: 对数据 `x` 进行两次随机增强，获得两个不同的视图 `x1` 和 `x2`。

4. **前向传播**:

   - `s1, s2 = gs(x1), gs(x2)`: 使用学生网络处理两个视图，得到输出 `s1` 和 `s2`。
   - `t1, t2 = gt(x1), gt(x2)`: 使用教师网络处理两个视图，得到输出 `t1` 和 `t2`。

5. **损失计算**:

   - 使用定义的损失函数 `H` 计算学生和教师网络的输出之间的差异。
   - `loss = H(t1, s2)/2 + H(t2, s1)/2`: 这是对称的损失函数，确保学生的一个输出与教师的另一个输出匹配，反之亦然。

6. **反向传播**:

   - `loss.backward()`: 根据损失反向传播，计算梯度。

7. **更新**:

   - `update(gs)`: 使用 SGD (随机梯度下降) 更新学生网络的参数。
   - `gt.params = l*gt.params + (1-l)*gs.params`: 使用动量 `l` 更新教师网络的参数。
   - `C = m*C + (1-m)*cat([t1, t2]).mean(dim=0)`: 更新中心 `C`。

8. **损失函数 H**:

   - 输入为教师 `t` 和学生 `s` 的输出。
   - 使用 softmax 函数处理两者，并进行必要的操作（例如，去除梯度、锐化等）。
   - 返回两者之间的负对数似然损失。

   

- #### github项目解读

  1. **.github/workflows**:
     - 这是GitHub Actions的配置目录，用于自动化任务，如测试、部署或文档生成。

  2. **dinov2**:
     - 项目的主要代码目录。通常，它包含模型定义、数据处理、训练循环等主要组件。

  3. **notebooks**:
     - 包含Jupyter笔记本，这些笔记本可能用于演示、教程或实验。

  4. **scripts**:
     - 通常包含一些实用脚本，如数据预处理、模型评估或任何其他与主要代码分开的任务。

  5. **.gitignore**:
     - 列出了不应该被Git跟踪或添加到版本控制中的文件和目录。

  6. **CODE_OF_CONDUCT.md**:
     - 为项目的贡献者和维护者设定行为准则。

  7. **CONTRIBUTING.md**:
     - 提供了为项目做贡献的指导，通常包括如何提交问题、创建分支和提交拉取请求等。

  8. **LICENSE**:
     - 描述了项目的许可证信息，表明其他人如何使用、修改或分发项目。

  9. **MODEL_CARD.md**:
     - 这是一个相对新的概念，用于详细描述机器学习模型的特性、使用方式、性能指标等。

  10. **README.md**:
     - 项目的主要文档，提供了关于项目的概述、如何安装、使用方法等。

  11. **conda-extras.yaml** 和 **conda.yaml**:
     - Conda环境的配置文件，列出了项目所需的所有依赖和包。

  12. **hubconf.py**:
     - 与PyTorch Hub相关，用于发布和共享预训练的PyTorch模型。

  13. **pyproject.toml**:
     - 包含Python项目和包的构建工具的配置信息。

  14. **requirements-dev.txt**, **requirements-extras.txt** 和 **requirements.txt**:
     - 列出项目所需的Python依赖。`requirements-dev.txt`通常包含开发和测试所需的额外依赖。

  15. **setup.cfg** 和 **setup.py**:
     - Python包的设置脚本，用于定义项目的元数据、依赖关系等，以便可以使用pip进行安装。

  

- #### **dinov2**文件解读

  1. **configs**:
     - 这个目录通常包含模型、数据集或训练的配置文件。它可能有一些预定义的配置，使得实验可以轻松地使用不同的超参数或模型结构。

  2. **data**:
     - 进行数据预处理、增强等。

  3. **distributed**:
     - 该目录包含与分布式训练相关的代码，特别是在多个GPU或多台机器上。

  4. **eval**:
     - 具体到各个下层应用的评估代码，包括语义分割，深度预测等。对于本模型的eval在run文件中。

  5. **fsdp**:
     - FSDP 是数据并行训练的一个扩展，允许将模型的参数、梯度和优化器状态在多个设备上进行分片，从而减少每个设备上的内存使用量。

  6. **hub**:
     - 用于定义模型的不同组件或模块。这些模块可以被组合在一起，与dino结合以形成完整的模型，用于不同的任务或实验。
     - **backbones.py**: 这个文件可能包含模型的基本结构部分，通常是模型的主要卷积部分。在许多深度学习模型中，背骨（backbone）用于从原始图像中提取特征，这些特征随后可以用于各种任务，如分类、目标检测等。
     - **classifiers.py**: 这个文件可能包含分类器部分的模型。通常，在提取了特征之后，分类器被用来预测一个或多个类别。
     - **depthers**:它与深度预测有关。可能是为了估计图像中物体的深度或3D结构。

  7. **layers**:
     - 这里包含定义的自定义层或模块，例如特殊的激活函数、正则化技术等。
  - **`attention.py`**: 这可能是实现了注意力机制的文件。注意力机制是深度学习中用于权重输入信息的技术，特别是在自然语言处理和计算机视觉的Transformer架构中。
     - **`block.py`**: 通常，这个文件可能包含定义模型中的一些重复使用的块或层的代码，如残差块或其他类型的模块。
     - **`dino_head.py`**: DINO是一个自监督学习的视觉表示方法，所以这个文件可能包含了与DINO相关的头部结构或层。
     - **`drop_path.py`**: 这可能是一个实现了路径丢弃技术的文件，该技术在某些网络架构中用于正则化。
     - **`layer_scale.py`**: 可能与层的缩放或归一化有关。
     - **`mlp.py`**: MLP代表多层感知机，是一个全连接的神经网络。此文件可能包含MLP的定义和实现。
     - **`patch_embed.py`**: 这可能是实现了将图像分割成多个小块（patches）并将其嵌入到特定的表示空间中的方法。
     - **`swiglu_ffn.py`**: 不太确定“swiglu”是什么，但“ffn”通常指前馈网络。可能是某种特定的网络结构或层。
     
  8. **logging**:
     - 与记录训练和评估日志有关的代码。这可能包括记录损失、准确性、学习率等的值。

  9. **loss**:
     - 定义用于训练模型的损失函数的代码。
  10. **models**:
     - 这里包含模型的结构和定义，可能有多种不同的模型结构或变种，用于版本更替。
  
  11. **run**:
     - 包含启动训练和评估的主要脚本。
  
12. **train**:
     - 这个目录包含模型训练的代码，包括前向传播、反向传播、优化器步骤等。
  
13. **utils**:
     - 通常包含各种实用程序和辅助函数，例如加载和保存模型、计算指标等。
     - **`cluster.py`**: 这个文件可能包含与聚类算法或操作有关的函数。聚类是无监督学习的一种，旨在将数据划分为若干个群组或“簇”。
- **`config.py`**: 这个文件通常用于定义和管理配置参数，如模型的超参数、训练设置或其他配置信息。
  - **`dtype.py`**: “dtype”通常指的是“data type”。这个文件可能包含与数据类型转换或管理有关的工具。
  - **`param_groups.py`**: 这可能与模型参数的分组有关。在深度学习中，我们有时会以不同的学习率更新不同的参数组，或对它们进行其他特殊处理。
  - **`utils.py`**: 这个文件可能是一个综合性的工具集，包含多种实用函数，支持整个项目的各种功能。
  
  14. **__init__.py**:
     - 这是一个空文件，使Python将此目录视为包含Python模块的包。
  

这些目录和文件为整个项目提供了一个清晰的组织结构，使得代码更容易理解和维护。不过，为了完全理解每个部分的具体功能和内容，直接查看其内部的代码和文档是最好的选择。


### train文件解读

- #### ssl_meta_arch代码解读

  - 这个方法是一个深度学习模型中的前向传播和反向传播函数，看起来是关于某种自监督学习任务的，具体是DINO (DIstillation of NOt-so-supervised representations) 和 IBOT。这个函数涉及的逻辑比较复杂，我会将其分成几个主要部分来解释。

    1. **初始化与参数校验**

    ```python
    n_global_crops = 2
    assert n_global_crops == 2
    n_local_crops = self.cfg.crops.local_crops_number
    
    global_crops = images["collated_global_crops"].cuda(non_blocking=True)
    local_crops = images["collated_local_crops"].cuda(non_blocking=True)
    ...
    ```
    在这一部分，方法主要进行了一些初始化操作和参数校验。它定义了两个全局的图像切割，确保其数量为2，并获取了局部的图像切割数量。此外，它还加载了各种输入图像和相关的遮罩到GPU中。

    2. **获取教师模型的输出**

       这段代码首先获取了教师模型的全局和局部特征。然后，根据`do_ibot`和`self.ibot_separate_head`的值，它决定如何处理这些特征：

       - 如果需要进行IBOT（对比学习方法，提取图像表征）且不使用单独的IBOT头部（`self.ibot_separate_head`为False），则将教师模型的全局类别特征（`teacher_cls_tokens`）和局部特征（`ibot_teacher_patch_tokens`）拼接到一起，然后通过DINO头部（`self.teacher.dino_head`）进行处理，分别得到`teacher_cls_tokens_after_head`和`masked_teacher_patch_tokens_after_head`。
       - 如果需要进行IBOT且使用单独的IBOT头部，则只处理局部特征，将局部特征通过IBOT头部（`self.teacher.ibot_head`）进行处理，得到`teacher_cls_tokens_after_head`和`masked_teacher_patch_tokens_after_head`。
       - 如果不需要IBOT，则只处理全局特征，将全局特征通过DINO头部进行处理，得到`teacher_cls_tokens_after_head`。

       接下来，根据训练配置中的`centering`选项，教师模型的输出进行了中心化处理。如果选择的是"centering"，则使用`self.dino_loss.softmax_center_teacher`或`self.ibot_patch_loss.softmax_center_teacher`进行中心化处理。如果选择的是"sinkhorn_knopp"，则使用`sinkhorn_knopp_teacher`进行中心化处理。

       最后，函数返回处理后的教师模型输出，用于后续的DINO和IBOT损失计算。整体而言，这个函数用于处理教师模型的输出，以便用于自监督学习任务中的损失计算。

    3. **计算损失**

    ```python
    loss_dict = {}
    loss_accumulator = 0
    student_global_backbone_output_dict, student_local_backbone_output_dict = self.student.backbone(...)
    ```
    这部分主要是计算学生模型的输出并与教师模型的输出进行比较以计算损失。损失是通过比较学生模型和教师模型的输出得到的，对于DINO和IBOT任务都有对应的损失。

    4. **处理局部切割图像的损失**

    ```python
    if n_local_crops > 0:
        ...
        loss_accumulator += self.dino_loss_weight * dino_local_crops_loss
    ```
    这部分处理了局部切割图像的DINO损失。如果存在局部切割，它会计算损失并添加到总损失累加器中。

    5. **处理全局切割图像的损失**

    ```python
    if do_dino:
        ...
        loss_accumulator += self.dino_loss_weight * dino_global_crops_loss
    if do_ibot:
        ...
        loss_accumulator += self.ibot_loss_weight * ibot_patch_loss
    ```
    这部分分别处理了全局切割图像的DINO和IBOT损失。

    6. **反向传播**

    ```python
    self.backprop_loss(loss_accumulator)
    ```
    最后，这部分使用累积的总损失进行反向传播，以更新学生模型的参数。

    整体来说，这个方法的目标是为了使用教师模型的输出来指导学生模型的训练，通过DINO和IBOT两种方法来计算损失，并使用这些损失来更新学生模型的参数。

- #### **do_train**代码解读

  ```python
  def do_train(cfg, model, resume=False):
  ```
  这是一个训练函数，它接受一个配置对象`cfg`，模型`model`，以及一个可选的`resume`参数来确定是否从之前的断点继续训练。

  ```python
      model.train()
  ```
  设置模型为训练模式。

  ```python
      inputs_dtype = torch.half
  ```
  定义输入数据的数据类型为半精度浮点数。

  ```python
      fp16_scaler = model.fp16_scaler  # for mixed precision training
  ```
  获取模型的混合精度训练缩放器，用于处理混合精度训练中的梯度。

  ```python
      optimizer = build_optimizer(cfg, model.get_params_groups())
  ```
  构建优化器。

  ```python
      (
          lr_schedule,
          wd_schedule,
          momentum_schedule,
          teacher_temp_schedule,
          last_layer_lr_schedule,
      ) = build_schedulers(cfg)
  ```
  构建学习率、权重衰减、动量、教师温度等的调度器。

  ```python
      checkpointer = FSDPCheckpointer(model, cfg.train.output_dir, optimizer=optimizer, save_to_disk=True)
  ```
  创建一个检查点管理器，用于保存和加载模型权重。

  ```python
      start_iter = checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get("iteration", -1) + 1
  ```
  从检查点加载模型，如果没有检查点，就从头开始。

  ```python
      OFFICIAL_EPOCH_LENGTH = cfg.train.OFFICIAL_EPOCH_LENGTH
      max_iter = cfg.optim.epochs * OFFICIAL_EPOCH_LENGTH
  ```
  定义每个epoch的长度和最大迭代次数。

  ```python
      periodic_checkpointer = PeriodicCheckpointer(
          checkpointer,
          period=3 * OFFICIAL_EPOCH_LENGTH,
          max_iter=max_iter,
          max_to_keep=3,
      )
  ```
  创建定期检查点保存器。

  ```python
      img_size = cfg.crops.global_crops_size
      patch_size = cfg.student.patch_size
      n_tokens = (img_size // patch_size) ** 2
      mask_generator = MaskingGenerator(
          input_size=(img_size // patch_size, img_size // patch_size),
          max_num_patches=0.5 * img_size // patch_size * img_size // patch_size,
      )
  ```
  设置图片尺寸，分块尺寸，并计算分块的数量。然后初始化一个分块生成器。

  ```python
      data_transform = DataAugmentationDINO(
          cfg.crops.global_crops_scale,
          cfg.crops.local_crops_scale,
          cfg.crops.local_crops_number,
          global_crops_size=cfg.crops.global_crops_size,
          local_crops_size=cfg.crops.local_crops_size,
      )
  ```
  设置数据增强方法。

  ```python
      collate_fn = partial(
          collate_data_and_cast,
          mask_ratio_tuple=cfg.ibot.mask_ratio_min_max,
          mask_probability=cfg.ibot.mask_sample_probability,
          n_tokens=n_tokens,
          mask_generator=mask_generator,
          dtype=inputs_dtype,
      )
  ```
  定义数据合并函数，它用于处理和转换批次数据。

  ```python
      dataset = make_dataset(
          dataset_str=cfg.train.dataset_path,
          transform=data_transform,
          target_transform=lambda _: (),
      )
      sampler_type = SamplerType.SHARDED_INFINITE
      data_loader = make_data_loader(
          dataset=dataset,
          batch_size=cfg.train.batch_size_per_gpu,
          num_workers=cfg.train.num_workers,
          shuffle=True,
          seed=start_iter,  # TODO: Fix this -- cfg.train.seed
          sampler_type=sampler_type,
          sampler_advance=0,  # TODO(qas): fix this -- start_iter * cfg.train.batch_size_per_gpu,
          drop_last=True,
          collate_fn=collate_fn,
      )
  ```
  初始化数据集和数据加载器。

  ```python
      iteration = start_iter
  ```
  设置当前迭代次数为开始迭代。

  ```python
      logger.info("Starting training from iteration {}".format(start_iter))
      metrics_file = os.path.join(cfg.train.output_dir, "training_metrics.json")
      metric_logger = MetricLogger(delimiter="  ", output_file=metrics_file)
      header = "Training"
  ```
  记录训练开始信息并初始化指标记录器。

  接下来是训练循环部分，其中包括：

  - 获取批次数据
  - 应用调度器更新学习率等参数
  - 计算损失并反向传播
  - 梯度裁剪
  - 更新模型权重
  - 执行教师的指数移动平均更新
  - 记录训练指标
  - 如果满足条件，则执行测试和保存模型的检查点

  这个训练循环会持续到达到最大迭代次数为止。

  ```python
      for data in metric_logger.log_every(
          data_loader,
          10,
          header,
          max_iter,
          start_iter,
      ):
          ...
  ```

  

### 实现

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from transformers import AutoImageProcessor, AutoModel
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
from PIL import Image, ImageDraw
import requests

COCO_CLASSES = [
    "__background__", "person", "bicycle", "car", "motorcycle", "airplane",
    "bus", "train", "truck", "boat", "traffic light", "fire hydrant", "N/A",
    "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep",
    "cow", "elephant", "bear", "zebra", "giraffe", "N/A", "backpack", "umbrella",
    "N/A", "N/A", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
    "sports ball", "kite", "baseball bat", "baseball glove", "skateboard",
    "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli",
    "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant",
    "bed", "N/A", "dining table", "N/A", "N/A", "toilet", "N/A", "tv", "laptop",
    "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster",
    "sink", "refrigerator", "N/A", "book", "clock", "vase", "scissors", "teddy bear",
    "hair drier", "toothbrush"
]


# Fetch the image
# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
# image = Image.open(requests.get(url, stream=True).raw)
image = Image.open("materials/images/屏幕截图 2023-10-12 140251.png").convert("RGB")

# Process the image and get the output from DINO model
processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')
model = AutoModel.from_pretrained('facebook/dinov2-base')

inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state

# Extract a feature map for visualization
feature_map = last_hidden_states[0, 0].detach().cpu().numpy()

# Resize this feature map to match the original image size
resized_feature_map = np.interp(feature_map, (feature_map.min(), feature_map.max()), (0, 255)).astype(np.uint8)
resized_feature_map = Image.fromarray(resized_feature_map).resize(image.size)

# Convert the resized_feature_map to 3 channels
resized_feature_map = np.stack([resized_feature_map]*3, axis=-1)

# Overlay the feature map on the original image
overlay = resized_feature_map * 0.5 + np.array(image) * 0.5

# Object Detection using Faster R-CNN
detector = fasterrcnn_resnet50_fpn(pretrained=True)
detector.eval()

# Convert image for Faster R-CNN
img_tensor = F.to_tensor(image).unsqueeze(0)

# Perform inference
with torch.no_grad():
    prediction = detector(img_tensor)

# Create a draw object to annotate the image with bounding boxes and labels
draw = ImageDraw.Draw(image)

# Draw bounding boxes on the overlay image
for box, score, label in zip(prediction[0]['boxes'], prediction[0]['scores'], prediction[0]['labels']):
    if score > 0.7:
        box = box.numpy().astype(int)
        label_str = f"Class: {COCO_CLASSES[label.item()]}, Score: {score.item():.2f}"
        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline="green", width=2)
        draw.text((box[0], box[1] - 20), label_str, fill="green")

# Display the image with bounding boxes and labels
plt.imshow(image)
plt.axis('off')
plt.show()
```

![image-20231024133127533](C:\Users\14815\AppData\Roaming\Typora\typora-user-images\image-20231024133127533.png)

```python
Time: 1.1984872817993164
```



#### 后续打算

将模型与gpt结合，做出可与图像模型对话交互的demo来应对初期答辩，如定向检测目标，切换检测模式等